# deep-learning-practice

Improving Neural Network Performance
1) Vanishing Gradients <br>
    $ Activation Function <br>
    $ Weight Initialization<br>
2) Overfitting<br>
    $ Reduce Complexity<br>
    $ Dropout Layers<br>
    $ Regularization<br>
    $ Early Stopping<br>
3) Normalization<br>
    $ Normalizing inputs<br>
    $ Batch Normalization<br>
    $ Normalizing Activations<br>
4) Gradient Checking and Clipping<br>
5) Optimizers-<br>
    $ Momentum<br>
    $ Adagrad<br>
    $ RMSprop<br>
    $ Adam<br>
6) Learning Rate Scheduling<br>
7) Hyperparameter Tuning<br>
   $ No. of hidden layers<br>
   $ nodes/layers<br>
   $ Batch size<br>
