# deep-learning-practice

Improving Neural Network Performance
1) Vanishing Gradients
    $ Activation Function
    $ Weight Initialization
2) Overfitting
    $ Reduce Complexity
    $ Dropout Layers
    $ Regularization
    $ Early Stopping
3) Normalization
    $ Normalizing inputs
    $ Batch Normalization
    $ Normalizing Activations
4) Gradient Checking and Clipping
5) Optimizers-
    $ Momentum
    $ Adagrad
    $ RMSprop
    $ Adam
6) Learning Rate Scheduling
7) Hyperparameter Tuning
   $ No. of hidden layers
   $ nodes/layers
   $ Batch size
